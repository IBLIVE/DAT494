{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6923af-48a8-4655-85e9-476f97b3b8f2",
   "metadata": {},
   "source": [
    "ChatGPT was used in this assignment to help generate the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59759ad5-3c12-495a-b390-5020263a37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c08f436-cd0d-4fb1-b098-0036e3df460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "838eccb4-d8d2-4518-b661-74803f790f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ammarsayedtaha/arabic-sign-language-dataset-2022?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 808M/808M [00:11<00:00, 73.9MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ikushbay/.cache/kagglehub/datasets/ammarsayedtaha/arabic-sign-language-dataset-2022/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"ammarsayedtaha/arabic-sign-language-dataset-2022\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c4900139-b9f3-4b1b-90ae-fc02d16f0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ikushbay/.cache/kagglehub/datasets/ammarsayedtaha/arabic-sign-language-dataset-2022/versions/2'\n",
    "\n",
    "batch_size = 64\n",
    "epochs     = 50\n",
    "lr         = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "695a8f17-fcaf-404f-b227-59ee4e0e1c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root   = os.path.join(path, \"datasets\")\n",
    "train_txt   = os.path.join(data_root, \"train.txt\")\n",
    "val_txt     = os.path.join(data_root, \"val.txt\")\n",
    "sign_yaml   = os.path.join(data_root, \"sign.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "22c49793-81ab-49ce-ad60-48f4da96a061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ain', 'al', 'aleff', 'bb', 'dal', 'dha', 'dhad', 'fa', 'gaaf', 'ghain', 'ha', 'haa', 'jeem', 'kaaf', 'khaa', 'la', 'laam', 'meem', 'nun', 'ra', 'saad', 'seen', 'sheen', 'ta', 'taa', 'thaa', 'thal', 'toot', 'waw', 'ya', 'yaa', 'zay']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import glob \n",
    "\n",
    "with open(sign_yaml, \"r\") as f:\n",
    "    meta = yaml.safe_load(f)\n",
    "class_names = meta[\"names\"]\n",
    "num_classes = len(class_names)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f6fcc867-dd85-4d7d-a270-98c145ebe508",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(128),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3, (0.5,)*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a09042a2-0b55-4a71-a629-9a39b8ee73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLangFlatDataset(Dataset):\n",
    "    def __init__(self, images_folder, class_names, transform=None):\n",
    "        self.transform   = transform\n",
    "        self.class_names = class_names\n",
    "        pattern = os.path.join(images_folder, \"*.*\")\n",
    "        self.paths = [p for p in glob.glob(pattern) if p.lower().endswith((\".jpg\",\".png\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        fname = os.path.basename(path)\n",
    "        # parse out the class token: last underscore-delimited field before the index\n",
    "        # e.g. \"1201_23_M_dhad_4.jpg\" → ['1201','23','M','dhad','4'] → take [-2]\n",
    "        cls_token = fname.rsplit(\".\",1)[0].split(\"_\")[-2]\n",
    "        label     = self.class_names.index(cls_token)\n",
    "\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2114780c-b250-49e3-a203-2c63e2eed5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 9955 | Val samples: 4247 | Classes: 32\n"
     ]
    }
   ],
   "source": [
    "train_img_dir = os.path.join(data_root, \"train\", \"images\")\n",
    "val_img_dir   = os.path.join(data_root, \"valid\", \"images\")\n",
    "\n",
    "train_ds = SignLangFlatDataset(train_img_dir, class_names, transform)\n",
    "val_ds   = SignLangFlatDataset(val_img_dir,   class_names, transform)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)} | Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4c5ab117-4409-43c4-b1d1-353305b08088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I decided to go with simple ResNet for this task\n",
    "class ToyResNet64(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        # helper: conv -> batchnorm -> relu\n",
    "        def conv_bn_relu(in_ch, out_ch, **kwargs):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, **kwargs),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        # Initial block: preserve 64x64, then downsample to 32x32\n",
    "        self.initial = nn.Sequential(\n",
    "            conv_bn_relu(3, 32, kernel_size=3, padding=1),  # -> 64x64\n",
    "            conv_bn_relu(32, 64, kernel_size=3, padding=1), # -> 64x64\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)           # -> 32x32\n",
    "        )\n",
    "\n",
    "        # Residual block 1 (32x32)\n",
    "        self.res1 = nn.Sequential(\n",
    "            conv_bn_relu(64, 64, kernel_size=3, padding=1),\n",
    "            conv_bn_relu(64, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "        # Residual block 2 (32x32)\n",
    "        self.res2 = nn.Sequential(\n",
    "            conv_bn_relu(64, 64, kernel_size=3, padding=1),\n",
    "            conv_bn_relu(64, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "        # Residual block 3 (32x32)\n",
    "        self.res3 = nn.Sequential(\n",
    "            conv_bn_relu(64, 64, kernel_size=3, padding=1),\n",
    "            conv_bn_relu(64, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        # Downsample again to 16x16\n",
    "        self.down = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Final conv -> 16x16\n",
    "        self.conv_final = conv_bn_relu(64, 64, kernel_size=3, padding=1)\n",
    "        self.gap        = nn.AdaptiveAvgPool2d(1)  # -> 1x1\n",
    "\n",
    "        # Classifier head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "        # He initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initial downsample\n",
    "        x = self.initial(x)\n",
    "        # first residual\n",
    "        b1 = x\n",
    "        x  = self.res1(x) + b1\n",
    "        # second residual\n",
    "        b2 = x\n",
    "        x  = self.res2(x) + b2\n",
    "        # third residual\n",
    "        b3 = x\n",
    "        x  = self.res3(x) + b3\n",
    "        # further downsampling\n",
    "        x  = self.down(x)\n",
    "        # final conv + pooling\n",
    "        x  = self.conv_final(x)\n",
    "        x  = self.gap(x).view(x.size(0), -1)\n",
    "        # classifier\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "76df825b-264b-4b8c-9dc3-38e6de53d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = ToyResNet(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "n_train, n_val = len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b218ff68-b866-4360-b798-23484eefa4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses, train_accs = [], []\n",
    "val_losses,   val_accs   = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "74b1af51-ec8e-48b5-aace-fd77a3cea650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50  Train: loss 3.4750, acc 0.0274  |  Val:   loss 3.4655, acc 0.0292\n",
      "Epoch 2/50  Train: loss 3.4666, acc 0.0302  |  Val:   loss 3.4654, acc 0.0318\n",
      "Epoch 3/50  Train: loss 3.4667, acc 0.0294  |  Val:   loss 3.4656, acc 0.0381\n",
      "Epoch 4/50  Train: loss 3.4664, acc 0.0303  |  Val:   loss 3.4647, acc 0.0360\n",
      "Epoch 5/50  Train: loss 3.4663, acc 0.0284  |  Val:   loss 3.4654, acc 0.0318\n",
      "Epoch 6/50  Train: loss 3.4662, acc 0.0294  |  Val:   loss 3.4654, acc 0.0318\n",
      "Epoch 7/50  Train: loss 3.4656, acc 0.0332  |  Val:   loss 3.4654, acc 0.0290\n",
      "Epoch 8/50  Train: loss 3.4660, acc 0.0296  |  Val:   loss 3.4652, acc 0.0318\n",
      "Epoch 9/50  Train: loss 3.4656, acc 0.0322  |  Val:   loss 3.4648, acc 0.0327\n",
      "Epoch 10/50  Train: loss 3.4653, acc 0.0287  |  Val:   loss 3.4641, acc 0.0334\n",
      "Epoch 11/50  Train: loss 3.4649, acc 0.0287  |  Val:   loss 3.4643, acc 0.0325\n",
      "Epoch 12/50  Train: loss 3.4643, acc 0.0298  |  Val:   loss 3.4638, acc 0.0318\n",
      "Epoch 13/50  Train: loss 3.4648, acc 0.0312  |  Val:   loss 3.4635, acc 0.0318\n",
      "Epoch 14/50  Train: loss 3.4640, acc 0.0319  |  Val:   loss 3.4639, acc 0.0301\n",
      "Epoch 15/50  Train: loss 3.4644, acc 0.0310  |  Val:   loss 3.4643, acc 0.0318\n",
      "Epoch 16/50  Train: loss 3.4631, acc 0.0349  |  Val:   loss 3.4642, acc 0.0327\n",
      "Epoch 17/50  Train: loss 3.4626, acc 0.0338  |  Val:   loss 3.4641, acc 0.0318\n",
      "Epoch 18/50  Train: loss 3.4617, acc 0.0364  |  Val:   loss 3.4643, acc 0.0400\n",
      "Epoch 19/50  Train: loss 3.4603, acc 0.0313  |  Val:   loss 3.4635, acc 0.0386\n",
      "Epoch 20/50  Train: loss 3.4600, acc 0.0374  |  Val:   loss 3.4635, acc 0.0367\n",
      "Epoch 21/50  Train: loss 3.4582, acc 0.0416  |  Val:   loss 3.4632, acc 0.0443\n",
      "Epoch 22/50  Train: loss 3.4561, acc 0.0380  |  Val:   loss 3.4618, acc 0.0384\n",
      "Epoch 23/50  Train: loss 3.4529, acc 0.0405  |  Val:   loss 3.4619, acc 0.0377\n",
      "Epoch 24/50  Train: loss 3.4471, acc 0.0399  |  Val:   loss 3.4613, acc 0.0363\n",
      "Epoch 25/50  Train: loss 3.4360, acc 0.0481  |  Val:   loss 3.4605, acc 0.0374\n",
      "Epoch 26/50  Train: loss 3.4161, acc 0.0468  |  Val:   loss 3.4623, acc 0.0398\n",
      "Epoch 27/50  Train: loss 3.4019, acc 0.0555  |  Val:   loss 3.4650, acc 0.0433\n",
      "Epoch 28/50  Train: loss 3.3904, acc 0.0542  |  Val:   loss 3.4625, acc 0.0487\n",
      "Epoch 29/50  Train: loss 3.3829, acc 0.0552  |  Val:   loss 3.4618, acc 0.0454\n",
      "Epoch 30/50  Train: loss 3.3687, acc 0.0570  |  Val:   loss 3.4560, acc 0.0513\n",
      "Epoch 31/50  Train: loss 3.3623, acc 0.0576  |  Val:   loss 3.4562, acc 0.0511\n",
      "Epoch 32/50  Train: loss 3.3541, acc 0.0634  |  Val:   loss 3.4498, acc 0.0560\n",
      "Epoch 33/50  Train: loss 3.3510, acc 0.0618  |  Val:   loss 3.4521, acc 0.0546\n",
      "Epoch 34/50  Train: loss 3.3462, acc 0.0634  |  Val:   loss 3.4447, acc 0.0556\n",
      "Epoch 35/50  Train: loss 3.3377, acc 0.0601  |  Val:   loss 3.4439, acc 0.0579\n",
      "Epoch 36/50  Train: loss 3.3358, acc 0.0629  |  Val:   loss 3.4458, acc 0.0556\n",
      "Epoch 37/50  Train: loss 3.3402, acc 0.0577  |  Val:   loss 3.4429, acc 0.0558\n",
      "Epoch 38/50  Train: loss 3.3383, acc 0.0643  |  Val:   loss 3.4376, acc 0.0563\n",
      "Epoch 39/50  Train: loss 3.3340, acc 0.0587  |  Val:   loss 3.4414, acc 0.0577\n",
      "Epoch 40/50  Train: loss 3.3317, acc 0.0628  |  Val:   loss 3.4373, acc 0.0584\n",
      "Epoch 41/50  Train: loss 3.3312, acc 0.0619  |  Val:   loss 3.4379, acc 0.0577\n",
      "Epoch 42/50  Train: loss 3.3329, acc 0.0686  |  Val:   loss 3.4370, acc 0.0567\n",
      "Epoch 43/50  Train: loss 3.3280, acc 0.0598  |  Val:   loss 3.4354, acc 0.0575\n",
      "Epoch 44/50  Train: loss 3.3331, acc 0.0658  |  Val:   loss 3.4354, acc 0.0565\n",
      "Epoch 45/50  Train: loss 3.3251, acc 0.0599  |  Val:   loss 3.4351, acc 0.0572\n",
      "Epoch 46/50  Train: loss 3.3283, acc 0.0647  |  Val:   loss 3.4352, acc 0.0560\n",
      "Epoch 47/50  Train: loss 3.3286, acc 0.0655  |  Val:   loss 3.4345, acc 0.0577\n",
      "Epoch 48/50  Train: loss 3.3238, acc 0.0653  |  Val:   loss 3.4342, acc 0.0577\n",
      "Epoch 49/50  Train: loss 3.3264, acc 0.0650  |  Val:   loss 3.4344, acc 0.0567\n",
      "Epoch 50/50  Train: loss 3.3274, acc 0.0651  |  Val:   loss 3.4341, acc 0.0563\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    # — Training —\n",
    "    model.train()\n",
    "    running_loss = running_acc = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss   = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        running_acc  += (logits.argmax(1) == labels).sum().item()\n",
    "    train_loss = running_loss / n_train\n",
    "    train_acc  = running_acc  / n_train\n",
    "\n",
    "    # — Record training metrics —\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "\n",
    "    # — Validation —\n",
    "    model.eval()\n",
    "    val_loss = val_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss   = criterion(logits, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            val_acc  += (logits.argmax(1) == labels).sum().item()\n",
    "    val_loss /= n_val\n",
    "    val_acc  /= n_val\n",
    "\n",
    "    # — Record validation metrics —\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # — Scheduler step and print —\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch {epoch}/{epochs}  \"\n",
    "          f\"Train: loss {train_loss:.4f}, acc {train_acc:.4f}  |  \"\n",
    "          f\"Val:   loss {val_loss:.4f}, acc {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab4d54a-c2fc-4351-a0b8-45f2a25dba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = range(1, epochs+1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs_range, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs_range, val_losses,   label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs_range, train_accs, label=\"Train Accuracy\")\n",
    "plt.plot(epochs_range, val_accs,   label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
